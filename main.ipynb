{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLX: 100%|██████████| 100/100 [04:22<00:00,  2.62s/it]\n",
      "Loft: 100%|██████████| 132/132 [00:27<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "def getOlx():\n",
    "    olx_list = []\n",
    "    for i in tqdm(range(1,101), desc='OLX'):\n",
    "        r = requests.get(f'https://www.olx.com.br/imoveis/estado-sp?o={i}', headers=headers)\n",
    "        content = r.content\n",
    "        soup = BeautifulSoup(r.content, 'html.parser') \n",
    "        li_tags = soup.find_all('li')\n",
    "        # Iterando sobre as tags <li> encontradas\n",
    "        for li_tag in li_tags:\n",
    "            # Verificando se a tag <li> contém um link\n",
    "            if li_tag.find('a'):\n",
    "                # Obtendo o valor do atributo \"href\" do link\n",
    "                link = li_tag.find('a')['href']        \n",
    "                if link.startswith('https://sp.olx.com.br'):\n",
    "                    olx_list.append(link)\n",
    "    return olx_list\n",
    "\n",
    "def getLoft():\n",
    "    temp = []\n",
    "    loft_list = []\n",
    "    for i in tqdm(range(1, 133),desc='Loft'):\n",
    "            response = requests.get(f'https://loft.com.br/venda/imoveis/sp/sao-paulo?utm_source=google&utm_medium=cpc&utm_campaign=ins_01_br_001_sp_0001_sao-paulo_all_aw_search_conversion_broad_&utm_content=all_loft-pura&utm_id=1756086660&utm_placement=&utm_ad_id=576438774862&gclid=Cj0KCQjwjryjBhD0ARIsAMLvnF9KvelK1iKF7xoTJ3HS0KSWMhoDO0UlANc1gTfQEMGg5Bud0HiPZg4aAscOEALw_wcB&pagina={i}')\n",
    "            html_content = response.text\n",
    "\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            divs = soup.find_all('div', class_='MuiGrid-root')\n",
    "\n",
    "            for div in divs:\n",
    "                tag_a = div.find('a')\n",
    "                if tag_a is not None:\n",
    "                    link = tag_a['href']\n",
    "                    if link.startswith('/imovel/'):\n",
    "                        temp.append(link)\n",
    "    loft_list = ['http://loft.com.br' + valor for valor in temp]\n",
    "    return loft_list\n",
    "\n",
    "## MAIN CODE\n",
    "olx_list = getOlx()\n",
    "loft_list = getLoft()\n",
    "\n",
    "## get olx data\n",
    "\n",
    "\n",
    "dados_gerais = []  # Lista para armazenar os dados de todas as URLs\n",
    "\n",
    "for url in tqdm(olx_list):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(r.content, 'lxml') \n",
    "\n",
    "    h2 = soup.find('h2', class_='ad__sc-1wimjbb-1 hoHpcC sc-cooIXK cXlgiS')\n",
    "    if h2:\n",
    "        h2_value = h2.text.strip()\n",
    "    else:\n",
    "        h2_value = ''\n",
    "\n",
    "    divs = soup.find_all('div', class_='sc-hmzhuo ad__sc-1f2ug0x-3 sSzeX sc-jTzLTM iwtnNi')  # Substitua 'nome_da_classe' pela classe real das divs\n",
    "\n",
    "    dados_url = {}  # Dicionário para armazenar os dados de uma URL específica\n",
    "\n",
    "    # Adicione a URL ao dicionário da URL específica com a chave 'URL'\n",
    "    dados_url['URL'] = url\n",
    "\n",
    "    # Adicione o valor da tag h2 ao dicionário da URL específica com a chave 'Valor'\n",
    "    dados_url['Valor'] = h2_value\n",
    "\n",
    "    # Itere sobre as divs encontradas\n",
    "    for div in divs:\n",
    "        # Inicialize um dicionário vazio para armazenar os valores da div atual\n",
    "        dados = {}\n",
    "\n",
    "        # Encontre todas as tags dt dentro da div atual\n",
    "        tags_dt = div.find_all('dt')\n",
    "\n",
    "        # Itere sobre as tags dt e obtenha os valores\n",
    "        for dt in tags_dt:\n",
    "            key = dt.text.strip()  # Use strip() para remover espaços em branco extras\n",
    "\n",
    "            # Encontre a próxima tag dd\n",
    "            dd = dt.find_next_sibling('dd')\n",
    "\n",
    "            # Se a tag dd existir, obtenha o valor\n",
    "            if dd:\n",
    "                value = dd.text.strip()\n",
    "            else:\n",
    "                # Se não houver uma tag dd, encontre a próxima div com a classe específica\n",
    "                proxima_div = div.find_next('div', class_='ad__sc-1f2ug0x-2 eSYIff')\n",
    "\n",
    "                # Encontre a tag a dentro da próxima div\n",
    "                a = proxima_div.find('a') if proxima_div else None\n",
    "\n",
    "                # Obtenha o valor da tag a, se existir\n",
    "                value = a.text.strip() if a else ''\n",
    "\n",
    "            dados[key] = value\n",
    "\n",
    "        # Adicione o dicionário da div atual ao dicionário da URL específica\n",
    "        dados_url.update(dados)\n",
    "\n",
    "    # Adicione o dicionário da URL específica à lista geral\n",
    "    dados_gerais.append(dados_url)\n",
    "\n",
    "# Converter a lista de dicionários em um DataFrame do pandas\n",
    "df = pd.DataFrame(dados_gerais)\n",
    "\n",
    "# Salvar o DataFrame em um arquivo Excel\n",
    "df.to_excel('dados.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOFT\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "dados_imoveis = []\n",
    "\n",
    "for url in tqdm(loft_list):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    desc = soup.find('h1', class_='MuiTypography-root jss197 jss174 jss183 MuiTypography-body1')\n",
    "    title = soup.find('title')\n",
    "\n",
    "    padrao_categoria = r\"^(\\w+)\"\n",
    "    padrao_tipo = r\"à\\s(\\w+)\"\n",
    "    padrao_bairro = r\"em\\s(.+?)\\scom\"\n",
    "    padrao_tamanho = r\"(\\d+)\\s*m²\"\n",
    "    padrao_quartos = r\"(\\d+)\\squartos\"\n",
    "    padrao_vaga = r\"(\\d+)\\svaga\"\n",
    "    padrao_valor = r\"R\\$\\s([\\d.,]+)\"\n",
    "\n",
    "    dados = {}\n",
    "\n",
    "    try:\n",
    "        dados['categoria'] = re.search(padrao_categoria, desc.text).group(1)\n",
    "    except:\n",
    "        dados['categoria'] = 'Unknow'\n",
    "    try:\n",
    "        dados['tipo'] = re.search(padrao_tipo, desc.text).group(1)\n",
    "    except:\n",
    "        dados['tipo'] = 'Unknow'\n",
    "    try:\n",
    "        dados['bairro'] = re.search(padrao_bairro, desc.text).group(1)\n",
    "    except:\n",
    "        dados['bairro'] = 'Unknow'\n",
    "    try:\n",
    "        dados['tamanho'] = re.search(padrao_tamanho, desc.text).group(1)\n",
    "    except:\n",
    "        dados['tamanho'] = '0'\n",
    "    try:\n",
    "        dados['quartos'] = re.search(padrao_quartos, desc.text).group(1)\n",
    "    except:\n",
    "        dados['quartos'] = '0'\n",
    "    try:\n",
    "        dados['vaga'] = re.search(padrao_vaga, desc.text).group(1)\n",
    "    except:\n",
    "        dados['vaga'] = '0'\n",
    "    try:\n",
    "        dados['valor'] = re.search(padrao_valor, title.text).group(1)\n",
    "    except:\n",
    "        dados['valor'] = '0'\n",
    "\n",
    "    dados['URL'] = url\n",
    "\n",
    "    dados_imoveis.append(dados)\n",
    "\n",
    "# Cria o DataFrame\n",
    "df = pd.DataFrame(dados_imoveis)\n",
    "\n",
    "# Define a ordem das colunas\n",
    "colunas = ['URL', 'Valor', 'CEP', 'Município', 'Bairro', 'Logradouro', 'Categoria', 'Vagas na garagem',\n",
    "           'Quartos', 'Acomoda', 'Características', 'Tipo', 'Área construída', 'Banheiros',\n",
    "           'Detalhes do imóvel', 'Condomínio', 'IPTU', 'Área útil', 'Detalhes do condomínio', 'Tamanho']\n",
    "\n",
    "# Reorganiza as colunas\n",
    "df = df[colunas]\n",
    "\n",
    "# Exporta o DataFrame para um arquivo Excel\n",
    "df.to_excel('dados_imoveis.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:02<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "## ZAP IMOVEIS\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "data = []\n",
    "\n",
    "for i in tqdm(range(1, 101)):\n",
    "    r = requests.get(f'https://www.zapimoveis.com.br/venda/apartamentos/?pagina={i}', headers=headers)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(r.content, 'html.parser') \n",
    "\n",
    "    price_p = soup.find_all('p', class_='simple-card__price js-price color-darker heading-regular heading-regular__bolder align-left')\n",
    "    size_value = soup.find_all('span', itemprop='floorSize')\n",
    "    rooms_value = soup.find_all('span', itemprop='numberOfRooms')\n",
    "    parking_value = soup.find_all('li', class_='feature__item text-small js-parking-spaces')\n",
    "    bathroom_value = soup.find_all('span', itemprop='numberOfBathroomsTotal')\n",
    "    address_value = soup.find_all('h2', class_='simple-card__address color-dark text-regular')\n",
    "    neighborhood_value = soup.find_all('h2', class_='simple-card__address color-dark text-regular')\n",
    "\n",
    "    for price, size, rooms, parking, bathroom, address, neighborhood in zip(price_p, size_value, rooms_value, parking_value, bathroom_value, address_value, neighborhood_value):\n",
    "        temp = {}  # Criar um novo dicionário temp em cada iteração\n",
    "\n",
    "        apt_value = price.find('strong')    \n",
    "        temp['price'] = apt_value.text.strip()\n",
    "        temp['size'] = size.text.strip()\n",
    "        temp['rooms'] = rooms.text.strip()\n",
    "        \n",
    "        try:\n",
    "            temp['parking'] = re.search(r'\\d+', parking.text).group()\n",
    "        except AttributeError:\n",
    "            temp['parking'] = '0'  # If parking info is not found, set it as 'N/A'\n",
    "        \n",
    "        try:\n",
    "            temp['bathroom'] = bathroom.text.strip()\n",
    "        except AttributeError:\n",
    "            temp['bathroom'] = '0'  # If bathroom info is not found, set it as 'N/A'\n",
    "        \n",
    "        temp['address'] = re.sub(r',.*', '',address.text).strip()\n",
    "        temp['neighborhood'] = re.sub(r'.*,', '', neighborhood.text).strip()\n",
    "        \n",
    "        data.append(temp)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('data-zap.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 278/278 [04:07<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "## vivareal\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "for i in tqdm(range(1, 279)):\n",
    "    r = requests.get(f'https://www.vivareal.com.br/venda/sp/sao-paulo/?pagina={i}', headers=headers)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(r.content, 'html.parser') \n",
    "    url_ad = soup.find_all('div', class_='property-card__main-link')\n",
    "    type_ad = soup.find_all('span', class_='property-card__title js-cardLink js-card-title')\n",
    "    price = soup.find_all('div', class_='property-card__price js-property-card-prices js-property-card__price-small')\n",
    "    neighborhood_value = soup.find_all('span', class_='property-card__address')\n",
    "    measure = soup.find_all('span', class_='property-card__detail-value js-property-card-value property-card__detail-area js-property-card-detail-area')\n",
    "    rooms = soup.find_all('li', class_='property-card__detail-item property-card__detail-room js-property-detail-rooms')\n",
    "    bathrooms = soup.find_all('li', class_='property-card__detail-item property-card__detail-bathroom js-property-detail-bathroom')\n",
    "    garage = soup.find_all('li', class_='property-card__detail-item property-card__detail-garage js-property-detail-garages')\n",
    "\n",
    "    for url, type_ad, price, neighborhood, measure, room, bathroom, garages in zip(url_ad, type_ad, price, neighborhood_value, measure, rooms, bathrooms, garage):\n",
    "        temp = {}\n",
    "        url_link = url.find('a')    \n",
    "        temp['URL'] = 'https://www.vivareal.com.br/imovel/' + url_link['href']\n",
    "        temp['Type'] = re.search(r'\\w+', type_ad.text).group()\n",
    "        temp['Price'] = price.text.strip()\n",
    "        temp['Address'] = neighborhood.text.strip()\n",
    "        temp['Measure'] = measure.text.strip()\n",
    "        temp['Rooms'] = room.text.strip()\n",
    "        temp['bathrooms'] = bathroom.text.strip()\n",
    "        temp['garage'] = garages.text.strip()\n",
    "        data.append(temp)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['Rooms'] = df['Rooms'].str.extract('(\\d+)')\n",
    "df['bathrooms'] = df['bathrooms'].str.extract('(\\d+)')\n",
    "df['garage'] = df['garage'].str.extract('(\\d+)')\n",
    "df['Price'] = df['Price'].str.replace('R\\$', '')\n",
    "df['Price'] = df['Price'].str.replace('.', '')\n",
    "df['Price'] = df['Price'].str.replace(' ', '')\n",
    "df['garage'] = df['garage'].fillna(0)\n",
    "df['Type'] = df['Type'].str.capitalize()\n",
    "df['Address'] = df['Address'].str.replace(', São Paulo.*', '')\n",
    "df['Address'] = df['Address'].str.replace('.*-', '')\n",
    "df['Measure'] = df['Measure'].str.replace('-.*', '')\n",
    "df['Price'] = pd.to_numeric(df['Price'])\n",
    "df['Measure'] = pd.to_numeric(df['Measure'])\n",
    "df['Rooms'] = pd.to_numeric(df['Rooms'])\n",
    "df['bathrooms'] = pd.to_numeric(df['bathrooms'])\n",
    "df['garage'] = pd.to_numeric(df['garage'])\n",
    "df.to_excel('data-vivareal.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data =[]\n",
    "\n",
    "for i in tqdm(range(1, 834)):\n",
    "    r = requests.get(f'https://emcasa.com/imoveis/sp/sao-paulo?pagina={i}')\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(r.content, 'html.parser') \n",
    "    url_ad = soup.find_all('a', class_='ListingCard_ecListingContainer__CBKdQ')\n",
    "    address = soup.find_all('p', class_='Typography_ecTypographyLabel__jXase Typography_ecTypographyRegular__FX_vf')\n",
    "    prices = soup.find_all('div', class_='ListingCard_ecListingCardPriceAndLabel__ZKCBN')\n",
    "    measure = soup.find_all('strong', class_='Typography_ecTypographyParagraphSmall__4HQRq Typography_ecTypographyBold__cxADa')\n",
    "    rooms = soup.find_all('p', class_='Typography_ecTypographyLabel__jXase Typography_ecTypographyRegular__FX_vf')\n",
    "    garages = soup.find_all('p', class_='Typography_ecTypographyLabel__jXase Typography_ecTypographyRegular__FX_vf')\n",
    "\n",
    "    temp_adress = []\n",
    "    temp_measure = []\n",
    "    temp_rooms = []\n",
    "    temp_garage = []\n",
    "\n",
    "    for address in address:\n",
    "        match = re.search(r'^(.*),\\s*São Paulo', address.text)    \n",
    "        if match:\n",
    "            result = match.group(1)\n",
    "            temp_adress.append(result)       \n",
    "        else:        \n",
    "            continue\n",
    "\n",
    "    for measure in measure:\n",
    "        match = re.search(r'(\\d+)\\s*m²', measure.text)\n",
    "        if match:\n",
    "            result = match.group(1)\n",
    "            temp_measure.append(result)             \n",
    "        else:        \n",
    "            continue\n",
    "\n",
    "    for room in rooms:\n",
    "        match = re.search(r'(\\d+)\\s*(?:quarto|quartos)', room.text)\n",
    "        \n",
    "        if match:\n",
    "            result = match.group(1)\n",
    "            temp_rooms.append(result)             \n",
    "        else:        \n",
    "            continue\n",
    "\n",
    "    count = 0\n",
    "    for garage in garages:\n",
    "        match = re.search(r'(\\d+)\\s*(?:vaga|vagas)', garage.text)   \n",
    "\n",
    "        if match:        \n",
    "            result = match.group(1)        \n",
    "            temp_garage.append(result)     \n",
    "            count = 0        \n",
    "        else:    \n",
    "            count += 1  \n",
    "            if count > 5:\n",
    "                temp_garage.append('0')\n",
    "                count = 0\n",
    "                continue\n",
    "\n",
    "    for url, price, address, measure, room, garage  in zip(url_ad, prices, temp_adress, temp_measure, temp_rooms, temp_garage):\n",
    "        temp = {}\n",
    "        temp['URL'] = 'https://emcasa.com' + url['href']\n",
    "        temp['Price'] = re.search(r'R\\$\\s*([0-9\\.]+),', price.text).group(1)\n",
    "        temp['Address'] = address\n",
    "        temp['Measure'] = measure\n",
    "        temp['Rooms'] = room\n",
    "        temp['garage'] = garage   \n",
    "        \n",
    "        data.append(temp)\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "df['Price'] = df['Price'].str.replace('.', '')\n",
    "df['Price'] = pd.to_numeric(df['Price'])\n",
    "df['Measure'] = pd.to_numeric(df['Measure'])\n",
    "df['Rooms'] = pd.to_numeric(df['Rooms'])\n",
    "df['garage'] = pd.to_numeric(df['garage'])\n",
    "df['Address'] = df['Address'].str.replace('.*,', '')\n",
    "\n",
    "df.to_excel('data-emcasa.xlsx', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39c5207e0ce1341ad898502cc592606dcf3036f0772b83709aba2273e6498e4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
